xgb_model = xgb.XGBClassifier(use_label_encoder=False,
                                  eval_metric='mlogloss',
                                  max_depth=6,  # Maximum depth of each tree
                                  learning_rate=0.3,  # Learning rate (eta)
                                  n_estimators=100,  # Number of trees in the forest
                                  colsample_bytree=1,  # Subsample ratio of columns when constructing each tree
                                  subsample=1,  # Subsample ratio of the training instances
                                  min_child_weight=1,  # Minimum sum of instance weight needed in a child
                                  gamma=0,  # Minimum loss reduction required to make a further partition on a leaf node
                                  reg_alpha=0,  # L1 regularization term on weights (alpha)
                                  reg_lambda=1  # L2 regularization term on weights (lambda))
    )

Classification report:
               precision    recall  f1-score   support

           0       0.78      0.28      0.41        50
           1       0.92      0.83      0.87       367
           2       0.00      0.00      0.00        30
           3       0.00      0.00      0.00        17
           4       0.67      0.31      0.42        65
           5       0.93      0.79      0.85        84
           6       0.00      0.00      0.00         3
           7       0.33      0.02      0.04        53
           8       0.00      0.00      0.00         5
           9       1.00      0.03      0.06        31
          10       0.00      0.00      0.00        10
          11       0.00      0.00      0.00         2
          12       0.83      0.86      0.85       131
          13       0.00      0.00      0.00         0
          14       0.00      0.00      0.00         1
          15       0.64      0.17      0.27        41
          16       0.59      0.12      0.20        83
          17       0.00      0.00      0.00         2
          18       0.00      0.00      0.00         0
          19       0.83      0.82      0.82       368
          20       0.00      0.00      0.00        52
          21       0.00      0.00      0.00         4
          22       0.00      0.00      0.00        36
          23       0.00      0.00      0.00         1
          24       0.00      0.00      0.00        14
          25       0.24      0.07      0.11       232
          26       0.33      0.02      0.04        48
          27       0.00      0.00      0.00         2
          28       0.00      0.00      0.00         3
          29       0.00      0.00      0.00         4
          30       0.00      0.00      0.00        39
          31       0.00      0.00      0.00         1
          32       0.00      0.00      0.00         5
          33       0.33      0.01      0.02        79
          34       1.00      0.06      0.11        36
          35       0.00      0.00      0.00         0
          36       0.89      0.88      0.89       298
          37       0.00      0.00      0.00         1
          38       0.50      0.11      0.18         9
          39       0.53      0.24      0.33       226
          40       0.62      0.37      0.46       401
          41       0.71      0.06      0.11        81
          42       0.00      0.00      0.00         2
          43       0.00      0.00      0.00         0
          44       0.00      0.00      0.00         0
          45       0.67      0.07      0.13        28
          46       0.00      0.00      0.00         0
          47       0.00      0.00      0.00         0
          48       0.00      0.00      0.00         0
          49       0.00      0.00      0.00         3
          50       0.00      0.00      0.00         1
          51       0.93      0.98      0.95       765
          52       0.00      0.00      0.00        57
          53       0.00      0.00      0.00         0

   micro avg       0.83      0.55      0.66      3771
   macro avg       0.26      0.13      0.15      3771
weighted avg       0.69      0.55      0.58      3771
 samples avg       0.82      0.68      0.71      3771

Accuracy: 0.468780487804878


# Initialize the XGBClassifier
    xgb_model = xgb.XGBClassifier(use_label_encoder=False,
                                  eval_metric='mlogloss',
                                  max_depth=6,  # Maximum depth of each tree
                                  learning_rate=0.3,  # Learning rate (eta)
                                  n_estimators=500,  # Number of trees in the forest
                                  colsample_bytree=1,  # Subsample ratio of columns when constructing each tree
                                  subsample=1,  # Subsample ratio of the training instances
                                  min_child_weight=1,  # Minimum sum of instance weight needed in a child
                                  gamma=0,  # Minimum loss reduction required to make a further partition on a leaf node
                                  reg_alpha=0,  # L1 regularization term on weights (alpha)
                                  reg_lambda=1  # L2 regularization term on weights (lambda))
    )

Classification report:
               precision    recall  f1-score   support

           0       0.79      0.30      0.43        50
           1       0.92      0.83      0.87       367
           2       0.00      0.00      0.00        30
           3       0.00      0.00      0.00        17
           4       0.67      0.28      0.39        65
           5       0.93      0.77      0.84        84
           6       0.00      0.00      0.00         3
           7       1.00      0.02      0.04        53
           8       0.00      0.00      0.00         5
           9       1.00      0.03      0.06        31
          10       0.00      0.00      0.00        10
          11       0.00      0.00      0.00         2
          12       0.83      0.85      0.84       131
          13       0.00      0.00      0.00         0
          14       0.00      0.00      0.00         1
          15       0.54      0.17      0.26        41
          16       0.64      0.11      0.19        83
          17       0.00      0.00      0.00         2
          18       0.00      0.00      0.00         0
          19       0.83      0.82      0.83       368
          20       1.00      0.02      0.04        52
          21       0.00      0.00      0.00         4
          22       0.00      0.00      0.00        36
          23       0.00      0.00      0.00         1
          24       0.00      0.00      0.00        14
          25       0.26      0.07      0.11       232
          26       0.50      0.04      0.08        48
          27       0.00      0.00      0.00         2
          28       0.00      0.00      0.00         3
          29       0.00      0.00      0.00         4
          30       0.00      0.00      0.00        39
          31       0.00      0.00      0.00         1
          32       0.00      0.00      0.00         5
          33       0.25      0.01      0.02        79
          34       1.00      0.06      0.11        36
          35       0.00      0.00      0.00         0
          36       0.90      0.87      0.89       298
          37       0.00      0.00      0.00         1
          38       0.50      0.11      0.18         9
          39       0.55      0.25      0.35       226
          40       0.65      0.40      0.50       401
          41       0.50      0.06      0.11        81
          42       0.00      0.00      0.00         2
          43       0.00      0.00      0.00         0
          44       0.00      0.00      0.00         0
          45       0.50      0.04      0.07        28
          46       0.00      0.00      0.00         0
          47       0.00      0.00      0.00         0
          48       0.00      0.00      0.00         0
          49       0.00      0.00      0.00         3
          50       0.00      0.00      0.00         1
          51       0.93      0.98      0.95       765
          52       0.00      0.00      0.00        57
          53       0.00      0.00      0.00         0

   micro avg       0.83      0.56      0.67      3771
   macro avg       0.29      0.13      0.15      3771
weighted avg       0.71      0.56      0.58      3771
 samples avg       0.83      0.68      0.72      3771

Accuracy: 0.4770731707317073

xgb_model = xgb.XGBClassifier(use_label_encoder=False,
                                  eval_metric='mlogloss',
                                  max_depth=6,  # Maximum depth of each tree
                                  learning_rate=0.1,  # Learning rate (eta)
                                  n_estimators=500,  # Number of trees in the forest
                                  colsample_bytree=1,  # Subsample ratio of columns when constructing each tree
                                  subsample=1,  # Subsample ratio of the training instances
                                  min_child_weight=1,  # Minimum sum of instance weight needed in a child
                                  gamma=0,  # Minimum loss reduction required to make a further partition on a leaf node
                                  reg_alpha=0,  # L1 regularization term on weights (alpha)
                                  reg_lambda=1  # L2 regularization term on weights (lambda))
    )

Classification report:
               precision    recall  f1-score   support

           0       0.78      0.28      0.41        50
           1       0.91      0.84      0.88       367
           2       0.00      0.00      0.00        30
           3       0.00      0.00      0.00        17
           4       0.71      0.31      0.43        65
           5       0.91      0.75      0.82        84
           6       0.00      0.00      0.00         3
           7       0.50      0.02      0.04        53
           8       0.00      0.00      0.00         5
           9       0.00      0.00      0.00        31
          10       0.00      0.00      0.00        10
          11       0.00      0.00      0.00         2
          12       0.83      0.89      0.86       131
          13       0.00      0.00      0.00         0
          14       0.00      0.00      0.00         1
          15       0.50      0.17      0.25        41
          16       0.47      0.10      0.16        83
          17       0.00      0.00      0.00         2
          18       0.00      0.00      0.00         0
          19       0.83      0.81      0.82       368
          20       1.00      0.02      0.04        52
          21       0.00      0.00      0.00         4
          22       0.00      0.00      0.00        36
          23       0.00      0.00      0.00         1
          24       0.00      0.00      0.00        14
          25       0.33      0.07      0.11       232
          26       0.00      0.00      0.00        48
          27       0.00      0.00      0.00         2
          28       0.00      0.00      0.00         3
          29       0.00      0.00      0.00         4
          30       0.00      0.00      0.00        39
          31       0.00      0.00      0.00         1
          32       0.00      0.00      0.00         5
          33       0.50      0.01      0.02        79
          34       1.00      0.03      0.05        36
          35       0.00      0.00      0.00         0
          36       0.90      0.88      0.89       298
          37       0.00      0.00      0.00         1
          38       1.00      0.11      0.20         9
          39       0.57      0.25      0.34       226
          40       0.65      0.37      0.47       401
          41       0.50      0.06      0.11        81
          42       0.00      0.00      0.00         2
          43       0.00      0.00      0.00         0
          44       0.00      0.00      0.00         0
          45       0.00      0.00      0.00        28
          46       0.00      0.00      0.00         0
          47       0.00      0.00      0.00         0
          48       0.00      0.00      0.00         0
          49       0.00      0.00      0.00         3
          50       0.00      0.00      0.00         1
          51       0.93      0.98      0.96       765
          52       1.00      0.02      0.03        57
          53       0.00      0.00      0.00         0

   micro avg       0.84      0.55      0.67      3771
   macro avg       0.27      0.13      0.15      3771
weighted avg       0.71      0.55      0.58      3771
 samples avg       0.84      0.68      0.72      3771

Accuracy: 0.4814634146341463



Classification report:
               precision    recall  f1-score   support

           0       0.83      0.81      0.82       368
           1       1.00      0.02      0.03        57
           2       0.65      0.37      0.47       401
           3       1.00      0.02      0.04        52
           4       1.00      0.11      0.20         9
           5       0.00      0.00      0.00         1
           6       0.00      0.00      0.00         0
           7       0.50      0.06      0.11        81
           8       0.00      0.00      0.00         0
           9       0.00      0.00      0.00         2
          10       0.00      0.00      0.00         0
          11       0.00      0.00      0.00         0
          12       1.00      0.03      0.05        36
          13       0.83      0.89      0.86       131
          14       0.47      0.10      0.16        83
          15       0.00      0.00      0.00         1
          16       0.00      0.00      0.00         3
          17       0.00      0.00      0.00         5
          18       0.00      0.00      0.00         1
          19       0.00      0.00      0.00        48
          20       0.00      0.00      0.00         2
          21       0.00      0.00      0.00        39
          22       0.71      0.31      0.43        65
          23       0.00      0.00      0.00        17
          24       0.00      0.00      0.00         2
          25       0.50      0.02      0.04        53
          26       0.00      0.00      0.00         3
          27       0.00      0.00      0.00        31
          28       0.00      0.00      0.00         4
          29       0.00      0.00      0.00         2
          30       0.00      0.00      0.00         0
          31       0.78      0.28      0.41        50
          32       0.90      0.88      0.89       298
          33       0.50      0.01      0.02        79
          34       0.00      0.00      0.00        36
          35       0.00      0.00      0.00         5
          36       0.91      0.84      0.88       367
          37       0.00      0.00      0.00         0
          38       0.00      0.00      0.00         1
          39       0.00      0.00      0.00         4
          40       0.00      0.00      0.00        28
          41       0.00      0.00      0.00         0
          42       0.00      0.00      0.00         0
          43       0.00      0.00      0.00        14
          44       0.33      0.07      0.11       232
          45       0.93      0.98      0.96       765
          46       0.91      0.75      0.82        84
          47       0.57      0.25      0.34       226
          48       0.00      0.00      0.00         0
          49       0.00      0.00      0.00         1
          50       0.50      0.17      0.25        41
          51       0.00      0.00      0.00        30
          52       0.00      0.00      0.00         3
          53       0.00      0.00      0.00        10

   micro avg       0.84      0.55      0.67      3771
   macro avg       0.27      0.13      0.15      3771
weighted avg       0.71      0.55      0.58      3771
 samples avg       0.84      0.68      0.72      3771


Classification report:
               precision    recall  f1-score   support

           0       0.00      0.00      0.00         0
           1       0.00      0.00      0.00         5
           2       0.00      0.00      0.00        10
           3       0.00      0.00      0.00        28
           4       0.00      0.00      0.00         0
           5       0.00      0.00      0.00         1
           6       0.96      0.21      0.34       131
           7       0.00      0.00      0.00        83
           8       0.00      0.00      0.00        31
           9       0.00      0.00      0.00        79
          10       1.00      0.02      0.04        50
          11       0.00      0.00      0.00         3
          12       0.00      0.00      0.00         0
          13       0.00      0.00      0.00         0
          14       0.00      0.00      0.00        48
          15       0.00      0.00      0.00        65
          16       0.00      0.00      0.00        36
          17       0.00      0.00      0.00         0
          18       0.00      0.00      0.00        36
          19       0.50      0.01      0.03       226
          20       0.93      0.22      0.36       367
          21       0.00      0.00      0.00        52
          22       0.00      0.00      0.00         2
          23       0.00      0.00      0.00         2
          24       1.00      0.11      0.20         9
          25       0.88      0.36      0.51       298
          26       0.25      0.01      0.02       232
          27       0.00      0.00      0.00        41
          28       0.00      0.00      0.00        17
          29       0.00      0.00      0.00        57
          30       0.54      0.13      0.21       368
          31       0.00      0.00      0.00         1
          32       0.00      0.00      0.00         1
          33       0.00      0.00      0.00        39
          34       0.90      0.81      0.85       765
          35       0.00      0.00      0.00        30
          36       0.50      0.04      0.07        84
          37       0.00      0.00      0.00         0
          38       0.00      0.00      0.00         4
          39       0.00      0.00      0.00        53
          40       0.00      0.00      0.00         0
          41       0.00      0.00      0.00         3
          42       0.00      0.00      0.00         2
          43       0.00      0.00      0.00         1
          44       0.00      0.00      0.00         5
          45       0.00      0.00      0.00         0
          46       0.00      0.00      0.00         1
          47       0.00      0.00      0.00         2
          48       0.50      0.01      0.02        81
          49       0.00      0.00      0.00        14
          50       0.00      0.00      0.00         4
          51       0.00      0.00      0.00         3
          52       0.61      0.07      0.13       401
          53       0.00      0.00      0.00         0

   micro avg       0.85      0.24      0.38      3771
   macro avg       0.16      0.04      0.05      3771
weighted avg       0.58      0.24      0.30      3771
 samples avg       0.44      0.34      0.36      3771
